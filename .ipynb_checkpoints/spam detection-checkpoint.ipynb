{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMS Spam detection using Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the code written in Python for building the a Spam Detector using Machine Learning trained on the 'spam.csv' file from Kaggle. We start from importing the dataset to building models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by importing the required libraries such as numpy, pandas, nltk, etc... then we import the dataset by using Pandas.\n",
    "The 'stopwords' contains the common English words that we don't really need for our algorithm and we stem each words. Then we convert the texts to a feature vector so that we can fit them to a classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the required library and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer,WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('spam.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('Unnamed: 0',axis=1)\n",
    "df = df.rename(columns={'v2': 'messages', 'v1': 'label'})\n",
    "df['label'] = df['label'].replace('ham',0)\n",
    "df['label'] = df['label'].replace('spam',1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us build a function that can clean the data by removing special characters, extra spaces and stopwords and stemming all the words in the text. We also remove the unecessary column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_data(data):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    ps = PorterStemmer()\n",
    "    wn = WordNetLemmatizer()\n",
    "    data = data.lower()\n",
    "    #Remove special characters\n",
    "    data = re.sub(r'[^0-9a-zA-Z]', ' ', data)\n",
    "    #Remove extra spaces\n",
    "    data = re.sub(r'\\s+', ' ', data)\n",
    "    #Remove stopwords\n",
    "    data = \" \".join(w for w in data.split() if w not in stop_words)\n",
    "    #stemming the words\n",
    "    #Perform Lemmatization then Stemming\n",
    "    words = data.split()\n",
    "    l = []\n",
    "    for w in words:\n",
    "        w = re.sub('ly$','',w)\n",
    "        w = wn.lemmatize(w,pos='v')\n",
    "        w = wn.lemmatize(w,pos='n')\n",
    "        w = wn.lemmatize(w,pos='a')\n",
    "        w = ps.stem(w)\n",
    "        l.append(w)\n",
    "    data = \" \".join(w for w in l)\n",
    "    #data = l\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['messages']=df['messages'].apply(clean_text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The class is imbalanced so the \"accuracy\" is not a good choice of classification metric\n",
    "# We will evaluate the model with \"precision\",\"recall\" and \"f1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X =df['messages']\n",
    "Y = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "vec = HashingVectorizer()\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline,make_pipeline\n",
    "f_selector = SelectFromModel(SGDClassifier(),threshold='mean')\n",
    "preprocessor = Pipeline([('Vectorizer',vec),('Feature_Selector',f_selector)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.2,stratify=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV,learning_curve\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from sklearn.pipeline import make_pipeline\n",
    "def plot_learning_curve(model,X_train,Y_train,metrics=\"f1\"):\n",
    "    N,train_score,val_score=learning_curve(model,X_train,Y_train,scoring=metrics)\n",
    "    plt.figure()\n",
    "    plt.plot(N,train_score.mean(axis=1),color='orange',label='training score')\n",
    "    plt.plot(N,val_score.mean(axis=1),color='blue',label='validation score')\n",
    "    plt.title('Learning curve')\n",
    "    plt.xlabel('Training Size')\n",
    "    plt.ylabel(metrics)\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "\n",
    "def tune_hyperparameters(model,X_train,Y_train,grid,metrics=\"f1\"):\n",
    "    gs = GridSearchCV(model,param_grid=grid,scoring=metrics)\n",
    "    gs.fit(X_train,Y_train)\n",
    "    print('Best Score: ',gs.best_score_)\n",
    "    print('Best parameters: ',gs.best_params_)\n",
    "    return gs.best_estimator_\n",
    "\n",
    "def classification_test(model,X_test,Y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    print('------ Evaluation on the test set-----')\n",
    "    print('CLASSIFICATION REPORT: \\n')\n",
    "    print(classification_report(Y_test,y_pred))\n",
    "    plt.figure(figsize=(3,3))\n",
    "    sns.heatmap(data=confusion_matrix(Y_test,y_pred),square=True,annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = make_pipeline(preprocessor,LogisticRegression(class_weight={0:1,1:3.89}))\n",
    "lr.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curve(lr,X_train,Y_train,metrics=\"recall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "classification_test(lr,X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = {'logisticregression__C': [1,2,5,8]}\n",
    "lr = tune_hyperparameters(lr,X_train,Y_train,grid,metrics='recall')\n",
    "plot_learning_curve(lr,X_train,Y_train,metrics=\"recall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_test(lr,X_test,Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dtc = make_pipeline(vec,f_selector,DecisionTreeClassifier(class_weight={0:1,1:3.89}))\n",
    "dtc.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_test(dtc,X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curve(dtc,X_train,Y_train,metrics='precision')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "grid = {'decisiontreeclassifier__max_depth': [1,5,10,20,25,30]}\n",
    "dtc = tune_hyperparameters(dtc,X_train,Y_train,grid,metrics='precision')\n",
    "plot_learning_curve(dtc,X_train,Y_train,metrics='precision')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_test(dtc,X_test,Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svm = make_pipeline(vec,f_selector,SVC(kernel='rbf',C=1,gamma=1,class_weight={0:1,1:3.89}))\n",
    "svm.fit(X_train,Y_train)\n",
    "plot_learning_curve(svm,X_train,Y_train,metrics='recall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = {'svc__C':[40,45,70],'svc__gamma':[0.01,0.005,0.05]}\n",
    "svm = tune_hyperparameters(svm,X_train,Y_train,grid,metrics='f1')\n",
    "plot_learning_curve(svm,X_train,Y_train,metrics='f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "classification_test(svm,X_test,Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "stc = StackingClassifier(estimators=[('LogisticRegression',lr),('SVM',svm),('DecisionTree',dtc)],final_estimator=KNeighborsClassifier())\n",
    "stc.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curve(stc,X_train,Y_train,'recall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_test(stc,X_test,Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "* The models didn't overfit the training set\n",
    "* The Support Vector Machine and Logistic Regression are our best candidate for this classification task\n",
    "* The Stacked model performs well above the three model trained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the model as a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def save_model(model,file_name):\n",
    "\tpickl = {'model':model}\t\n",
    "\tpickle.dump(pickl,open(file_name+\".p\",\"wb\"))\n",
    "    \n",
    "def load_models(file_name):\n",
    "\twith open(file_name,'rb') as pickled:\n",
    "\t\tdata = pickle.load(pickled)\n",
    "\t\tmodel = data['model']\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to save the model by building a pipeline of combining the \"Vectorizer\" with the \"Models\" because we need it later for building the web app\n",
    "Now lets build a \"Function transformer\" for the SCIKIT-LEARN MODEL capable of cleaning all the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "def text_cleaner(text):\n",
    "    return [clean_text_data(text)]\n",
    "\n",
    "cleaner = FunctionTransformer(func=text_cleaner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model1 = Pipeline([('Text cleaner',cleaner),('LR_Pipeline',lr)])\n",
    "final_model2 = Pipeline([('Text cleaner',cleaner),('SVM_Pipeline',svm)])\n",
    "final_model3 = Pipeline([('Text cleaner',cleaner),('Stack_Pipeline',stc)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(final_model1,'lr')\n",
    "save_model(final_model2,'svm')\n",
    "save_model(final_model3,'stc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.to_csv('X_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
